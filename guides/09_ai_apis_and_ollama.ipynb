{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfecto ‚úÖ\n",
    "Aqu√≠ tienes la **traducci√≥n literal al espa√±ol** del texto completo, manteniendo el formato Markdown, sin a√±adir ni omitir nada, y respetando la estructura y los fragmentos de c√≥digo originales:\n",
    "\n",
    "---\n",
    "\n",
    "# APIs de LLM y Ollama - m√°s all√° de OpenAI\n",
    "\n",
    "*IMPORTANTE: Si no est√°s tan familiarizado con las APIs en general y con las Variables de Entorno en tu PC o Mac, por favor revisa la secci√≥n de APIs en la Gu√≠a 4 Fundamentos T√©cnicos antes de continuar con esta gu√≠a (temas 3 y 5 en la Gu√≠a 4).*\n",
    "\n",
    "## Contexto crucial para usar modelos distintos de OpenAI - por favor, lee esto primero\n",
    "\n",
    "A lo largo del curso, utilizamos APIs para conectarnos con los LLMs m√°s potentes del planeta.\n",
    "\n",
    "Las empresas detr√°s de estos LLMs, como OpenAI, Anthropic, Google y DeepSeek, han creado endpoints web. Llamas a sus modelos realizando una solicitud HTTP a una direcci√≥n web y pasando toda la informaci√≥n sobre tus *prompts*.\n",
    "\n",
    "Pero ser√≠a tedioso si tuvi√©ramos que construir solicitudes HTTP cada vez que quisi√©ramos llamar a una API.\n",
    "\n",
    "Para simplificar esto, el equipo de OpenAI escribi√≥ una utilidad en Python conocida como una ‚ÄúLibrer√≠a Cliente de Python‚Äù, que envuelve la llamada HTTP. As√≠, escribes c√≥digo en Python y este llama a la web.\n",
    "\n",
    "Y ESO es lo que es la librer√≠a `openai`.\n",
    "\n",
    "### Qu√© es la librer√≠a cliente de Python `openai`\n",
    "\n",
    "Es:\n",
    "\n",
    "* Una utilidad ligera de Python\n",
    "* Convierte tus solicitudes en Python en una llamada HTTP\n",
    "* Convierte los resultados que vienen de la llamada HTTP en objetos de Python\n",
    "\n",
    "### Qu√© NO es\n",
    "\n",
    "* No contiene ning√∫n c√≥digo para ejecutar realmente un Modelo de Lenguaje Grande (LLM). ¬°No hay c√≥digo GPT! Solo realiza una solicitud web.\n",
    "* No tiene c√≥digo de computaci√≥n cient√≠fica ni nada particularmente especializado para OpenAI\n",
    "\n",
    "### C√≥mo usarla:\n",
    "\n",
    "```python\n",
    "# Crear un cliente de OpenAI en Python para realizar llamadas web a OpenAI\n",
    "openai = OpenAI()\n",
    "\n",
    "# Hacer la llamada\n",
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=[{\"role\":\"user\", \"content\": \"what is 2+2?\"}])\n",
    "\n",
    "# Imprimir el resultado\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "### Qu√© hace esto\n",
    "\n",
    "Cuando haces la llamada en Python: `openai.chat.completions.create()`\n",
    "Simplemente realiza una solicitud web a esta URL: `https://api.openai.com/v1/chat/completions`\n",
    "Y convierte la respuesta en objetos de Python.\n",
    "\n",
    "Eso es todo.\n",
    "\n",
    "Aqu√≠ est√° la documentaci√≥n de la API si haces [llamadas HTTP directas por web](https://platform.openai.com/docs/guides/text?api-mode=chat&lang=curl)\n",
    "Y aqu√≠ est√° la misma documentaci√≥n de la API si usas la [Librer√≠a Cliente de Python](https://platform.openai.com/docs/guides/text?api-mode=chat&lang=python)\n",
    "\n",
    "## Con ese contexto - ¬øc√≥mo uso otros LLMs?\n",
    "\n",
    "Resulta que... ¬°es s√∫per f√°cil!\n",
    "\n",
    "Todos los dem√°s LLMs principales tienen endpoints API que son compatibles con OpenAI.\n",
    "\n",
    "Y entonces OpenAI hizo un favor a todos: dijeron, mirad, pod√©is usar nuestra utilidad para convertir Python en solicitudes web. Permitiremos que cambi√©is la utilidad de llamar a `https://api.openai/com/v1` a llamar a cualquier direcci√≥n web que especifiqu√©is.\n",
    "\n",
    "Y as√≠ puedes usar la utilidad de OpenAI incluso para llamar a modelos que NO son de OpenAI, como esto:\n",
    "\n",
    "`not_actually_openai = OpenAI(base_url=\"https://somewhere.completely.different/\", api_key=\"another_providers_key\")`\n",
    "\n",
    "Es importante entender que este c√≥digo de OpenAI es solo una utilidad para realizar llamadas HTTP a endpoints. As√≠ que, aunque estemos usando c√≥digo del equipo de OpenAI, podemos usarlo para llamar a modelos distintos de OpenAI.\n",
    "\n",
    "Aqu√≠ est√°n todos los endpoints compatibles con OpenAI de los principales proveedores. Incluso incluye el uso de Ollama, de forma local. Ollama proporciona un endpoint en tu m√°quina local, y tambi√©n lo hizo compatible con OpenAI: muy conveniente.\n",
    "\n",
    "```python\n",
    "ANTHROPIC_BASE_URL = \"https://api.anthropic.com/v1/\"\n",
    "DEEPSEEK_BASE_URL = \"https://api.deepseek.com/v1\"\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "GROK_BASE_URL = \"https://api.x.ai/v1\"\n",
    "GROQ_BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "```\n",
    "\n",
    "## Aqu√≠ tienes ejemplos para Gemini, DeepSeek, Ollama y OpenRouter\n",
    "\n",
    "### Ejemplo 1: Usar Gemini en lugar de OpenAI\n",
    "\n",
    "1. Visita Google Studio para crear una cuenta: [https://aistudio.google.com/](https://aistudio.google.com/)\n",
    "2. A√±ade tu clave como GOOGLE_API_KEY a tu archivo `.env`\n",
    "3. A√±√°dela tambi√©n una segunda vez como GEMINI_API_KEY a tu `.env` ‚Äî esto ser√° √∫til m√°s adelante.\n",
    "\n",
    "Luego:\n",
    "\n",
    "```python\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n",
    "response = gemini.chat.completions.create(model=\"gemini-2.5-flash-preview-05-20\", messages=[{\"role\":\"user\", \"content\": \"what is 2+2?\"}])\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "### Ejemplo 2: Usar la API de DeepSeek en lugar de OpenAI (barato, y solo $2 de dep√≥sito inicial)\n",
    "\n",
    "1. Visita la API de DeepSeek para crear una cuenta: [https://platform.deepseek.com/](https://platform.deepseek.com/)\n",
    "2. Necesitar√°s a√±adir un saldo inicial m√≠nimo de $2.\n",
    "3. A√±ade tu clave como DEEPSEEK_API_KEY a tu archivo `.env`\n",
    "\n",
    "Luego:\n",
    "\n",
    "```python\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "DEEPSEEK_BASE_URL = \"https://api.deepseek.com/v1\"\n",
    "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "deepseek = OpenAI(base_url=DEEPSEEK_BASE_URL, api_key=deepseek_api_key)\n",
    "response = deepseek.chat.completions.create(model=\"deepseek-chat\", messages=[{\"role\":\"user\", \"content\": \"what is 2+2?\"}])\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "### Ejemplo 3: Usar Ollama para que sea gratuito y local en lugar de OpenAI\n",
    "\n",
    "Ollama te permite ejecutar modelos localmente; proporciona una API compatible con OpenAI en tu m√°quina.\n",
    "No hay clave de API para Ollama; no hay terceros con tu tarjeta de cr√©dito, por lo que no se necesita ning√∫n tipo de clave.\n",
    "\n",
    "1. Si eres nuevo en Ollama, inst√°lalo siguiendo las instrucciones aqu√≠: [https://ollama.com](https://ollama.com)\n",
    "2. Luego, en una terminal de Cursor, ejecuta `ollama run llama3.2` para chatear con Llama 3.2\n",
    "   CUIDADO: no uses llama3.3 o llama4 ‚Äî ¬°estos son modelos masivos no dise√±ados para ordenadores dom√©sticos! Llenar√°n tu disco.\n",
    "\n",
    "Luego:\n",
    "\n",
    "```python\n",
    "!ollama pull llama3.2\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"anything\")\n",
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=[{\"role\":\"user\", \"content\": \"what is 2+2?\"}])\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "### Ejemplo 4: Usar el popular servicio [OpenRouter](https://openrouter.ai), que tiene un proceso de facturaci√≥n m√°s sencillo, en lugar de OpenAI\n",
    "\n",
    "OpenRouter es muy conveniente: te da acceso gratuito a muchos modelos y acceso f√°cil con un peque√±o pago inicial a modelos de pago.\n",
    "\n",
    "1. Reg√≠strate en [https://openrouter.ai](https://openrouter.ai)\n",
    "2. A√±ade el saldo m√≠nimo inicial seg√∫n sea necesario\n",
    "3. A√±ade tu clave como OPENROUTER_API_KEY a tu archivo `.env`\n",
    "\n",
    "Luego:\n",
    "\n",
    "```python\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "openrouter = OpenAI(base_url=OPENROUTER_BASE_URL, api_key=openrouter_api_key)\n",
    "response = openrouter.chat.completions.create(model=\"openai/gpt-4.1-nano\", messages=[{\"role\":\"user\", \"content\": \"what is 2+2?\"}])\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "### Usar diferentes proveedores de API con frameworks de agentes\n",
    "\n",
    "Los frameworks de agentes hacen que sea f√°cil cambiar entre estos proveedores. Puedes intercambiar LLMs y elegir diferentes en cualquier momento del curso. Hay m√°s notas sobre cada uno de ellos m√°s abajo. Para el SDK de OpenAI Agents, consulta una secci√≥n m√°s adelante en este cuaderno. Para CrewAI, lo cubrimos en el curso, pero es f√°cil: simplemente usa la ruta completa al modelo que espera LiteLLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costos de las APIs\n",
    "\n",
    "El costo de cada llamada a una API es realmente muy bajo; la mayor√≠a de las llamadas a los modelos que usamos en este curso cuestan fracciones de centavo.\n",
    "\n",
    "Pero es extremadamente importante tener en cuenta:\n",
    "\n",
    "1. Un proyecto agentizado complejo podr√≠a implicar muchas llamadas a LLMs ‚Äîquiz√°s entre 20 y 30‚Äî, por lo que los costos pueden acumularse. Es importante establecer l√≠mites y supervisar el uso.\n",
    "\n",
    "2. Con la IA agentizada, existe el riesgo de que los agentes entren en un bucle o realicen m√°s procesamiento del previsto. Debes supervisar tu uso de la API y nunca asignar m√°s presupuesto del que te sientas c√≥modo. Algunas APIs tienen una opci√≥n de ‚Äúrecarga autom√°tica‚Äù que puede cargar dinero autom√°ticamente a tu tarjeta ‚Äîte recomiendo encarecidamente mantenerla desactivada.\n",
    "\n",
    "3. Solo deber√≠as gastar lo que te resulte c√≥modo. Existe una alternativa gratuita en Ollama que puedes usar como reemplazo si lo deseas. DeepSeek, Gemini 2.5 Flash y gpt-4.1-nano son significativamente m√°s econ√≥micos.\n",
    "\n",
    "Ten en cuenta que estas llamadas a LLM suelen implicar billones de c√°lculos de coma flotante ‚Äî¬°alguien tiene que pagar las facturas de electricidad! ‚ö°üí∞\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama: alternativa gratuita a las APIs de pago (pero por favor, lee la advertencia sobre la versi√≥n de llama)\n",
    "\n",
    "Ollama es un producto que se ejecuta localmente en tu ordenador. Puede ejecutar modelos de c√≥digo abierto y proporciona un endpoint de API en tu equipo que es compatible con OpenAI.\n",
    "\n",
    "Primero, descarga Ollama visitando:\n",
    "[https://ollama.com](https://ollama.com)\n",
    "\n",
    "Luego, desde tu Terminal en Cursor (men√∫ **View >> Terminal**), ejecuta este comando para descargar un modelo:\n",
    "\n",
    "```shell\n",
    "ollama pull llama3.2\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è **ADVERTENCIA:** Ten cuidado de no usar *llama3.3* o *llama4* ‚Äîestos son modelos mucho m√°s grandes que no son adecuados para ordenadores dom√©sticos.\n",
    "\n",
    "Y ahora, cada vez que tengamos c√≥digo como:\n",
    "`openai = OpenAI()`\n",
    "puedes usar esto como reemplazo directo:\n",
    "`openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')`\n",
    "Y tambi√©n reemplazar nombres de modelos como **gpt-4o-mini** por **llama3.2**.\n",
    "\n",
    "No necesitas poner nada en tu archivo `.env` para esto; con Ollama, todo se ejecuta en tu propio ordenador. No est√°s haciendo llamadas a un tercero en la nube, nadie tiene los datos de tu tarjeta de cr√©dito, ¬°as√≠ que no necesitas ninguna clave secreta! El c√≥digo `api_key='ollama'` que aparece arriba solo es necesario porque la librer√≠a cliente de OpenAI espera que se le pase un par√°metro `api_key`, pero Ollama ignora ese valor.\n",
    "\n",
    "A continuaci√≥n se muestra un ejemplo completo:\n",
    "\n",
    "```python\n",
    "# Necesitas hacer esto una sola vez en tu ordenador\n",
    "!ollama pull llama3.2\n",
    "\n",
    "from openai import OpenAI\n",
    "MODEL = \"llama3.2\"\n",
    "openai = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    " model=MODEL,\n",
    " messages=[{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "Necesitar√°s hacer cambios similares para usar Ollama dentro de cualquiera de los frameworks de agentes; deber√≠as poder buscar un ejemplo exacto en Google o preguntarme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenRouter: plataforma de acceso conveniente para OpenAI y otros\n",
    "\n",
    "OpenRouter es un servicio de terceros que te permite conectarte a una amplia gama de LLMs, incluidos los de OpenAI.\n",
    "\n",
    "Es conocido por tener un proceso de facturaci√≥n m√°s simple, que puede resultar m√°s c√≥modo para algunos pa√≠ses fuera de Estados Unidos.\n",
    "\n",
    "Primero, visita su sitio web:\n",
    "[https://openrouter.ai/](https://openrouter.ai/)\n",
    "\n",
    "Luego, echa un vistazo a su gu√≠a r√°pida:\n",
    "[https://openrouter.ai/docs/quickstart](https://openrouter.ai/docs/quickstart)\n",
    "\n",
    "Y a√±ade tu clave a tu archivo `.env`:\n",
    "\n",
    "```shell\n",
    "OPENROUTER_API_KEY=sk-or....\n",
    "```\n",
    "\n",
    "Y ahora, cada vez que tengas c√≥digo como este:\n",
    "\n",
    "```python\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "openai = OpenAI()\n",
    "```\n",
    "\n",
    "Puedes reemplazarlo por este c√≥digo:\n",
    "\n",
    "```python\n",
    "MODEL = \"openai/gpt-4o-mini\"\n",
    "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "openai = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=openrouter_api_key)\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    " model=MODEL,\n",
    " messages=[{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "Necesitar√°s hacer cambios similares para usar OpenRouter dentro de cualquiera de los frameworks de agentes; deber√≠as poder buscar un ejemplo exacto en Google o preguntarme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Agents SDK - instrucciones espec√≠ficas\n",
    "\n",
    "Con el **OpenAI Agents SDK** (semanas 2 y 6), es especialmente f√°cil usar cualquier modelo ofrecido por OpenAI.\n",
    "Simplemente pasa el nombre del modelo as√≠:\n",
    "\n",
    "```python\n",
    "agent = Agent(name=\"Jokester\", instructions=\"You are a joke teller\", model=\"gpt-4o-mini\")\n",
    "```\n",
    "\n",
    "Tambi√©n puedes sustituirlo por cualquier otro proveedor que tenga una API compatible con OpenAI.\n",
    "Se hace en **3 pasos** como este:\n",
    "\n",
    "```python\n",
    "DEEPSEEK_BASE_URL = \"https://api.deepseek.com/v1\"\n",
    "deepseek_client = AsyncOpenAI(base_url=DEEPSEEK_BASE_URL, api_key=deepseek_api_key)\n",
    "deepseek_model = OpenAIChatCompletionsModel(model=\"deepseek-chat\", openai_client=deepseek_client)\n",
    "```\n",
    "\n",
    "Y luego simplemente proporcionas este modelo al crear un agente:\n",
    "\n",
    "```python\n",
    "agent = Agent(name=\"Jokester\", instructions=\"You are a joke teller\", model=deepseek_model)\n",
    "```\n",
    "\n",
    "Puedes usar un enfoque similar con cualquier otra API compatible con OpenAI, siguiendo los mismos **3 pasos**:\n",
    "\n",
    "```python\n",
    "# Importaciones adicionales\n",
    "from agents import OpenAIChatCompletionsModel\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Paso 1: especificar el endpoint base donde el proveedor ofrece una API compatible con OpenAI\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "GROK_BASE_URL = \"https://api.x.ai/v1\"\n",
    "GROQ_BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "# Paso 2: crear un objeto AsyncOpenAI para ese endpoint\n",
    "gemini_client = AsyncOpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n",
    "grok_client = AsyncOpenAI(base_url=GROK_BASE_URL, api_key=grok_api_key)\n",
    "groq_client = AsyncOpenAI(base_url=GROQ_BASE_URL, api_key=groq_api_key)\n",
    "openrouter_client = AsyncOpenAI(base_url=OPENROUTER_BASE_URL, api_key=openrouter_api_key)\n",
    "ollama_client = AsyncOpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")\n",
    "\n",
    "# Paso 3: crear un objeto de modelo para usar al crear el agente\n",
    "gemini_model = OpenAIChatCompletionsModel(model=\"gemini-2.5-flash\", openai_client=gemini_client)\n",
    "grok_3_model = OpenAIChatCompletionsModel(model=\"grok-3-mini-beta\", openai_client=openrouter_client)\n",
    "llama3_3_model = OpenAIChatCompletionsModel(model=\"llama-3.3-70b-versatile\", openai_client=groq_client)\n",
    "grok_3_via_openrouter_model = OpenAIChatCompletionsModel(model=\"x-ai/grok-3-mini-beta\", openai_client=openrouter_client)\n",
    "llama_3_2_local_model = OpenAIChatCompletionsModel(model=\"llama3.2\", openai_client=ollama_client)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ü™ü C√≥mo usar **Azure** con OpenAI Agents SDK\n",
    "\n",
    "Consulta las instrucciones aqu√≠:\n",
    "üëâ [Use Azure OpenAI and APIM with the OpenAI Agents SDK](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/use-azure-openai-and-apim-with-the-openai-agents-sdk/4392537)\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "```python\n",
    "from openai import AsyncAzureOpenAI\n",
    "from agents import set_default_openai_client\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    " \n",
    "# Cargar las variables de entorno\n",
    "load_dotenv()\n",
    " \n",
    "# Crear el cliente de OpenAI usando Azure OpenAI\n",
    "openai_client = AsyncAzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    ")\n",
    " \n",
    "# Establecer el cliente de OpenAI por defecto para el SDK de agentes\n",
    "set_default_openai_client(openai_client)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuraci√≥n de CrewAI\n",
    "\n",
    "Aqu√≠ tienes la documentaci√≥n de Crew para conexiones con LLMs, incluyendo los nombres de modelo que debes usar para todos los modelos. Como se√±al√≥ el estudiante Sadan S. (¬°gracias!), es importante saber que para Google necesitas usar la variable de entorno `GEMINI_API_KEY` en lugar de `GOOGLE_API_KEY`:\n",
    "\n",
    "[https://docs.crewai.com/concepts/llms](https://docs.crewai.com/concepts/llms)\n",
    "\n",
    "Y aqu√≠ tienes su tutorial con m√°s informaci√≥n:\n",
    "\n",
    "[https://docs.crewai.com/how-to/llm-connections](https://docs.crewai.com/how-to/llm-connections)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuraci√≥n de LangGraph\n",
    "\n",
    "Para usar **LangGraph** con **Ollama** (y de forma similar con otros modelos):\n",
    "[https://python.langchain.com/docs/integrations/chat/ollama/#installation](https://python.langchain.com/docs/integrations/chat/ollama/#installation)\n",
    "\n",
    "Primero, a√±ade el paquete:\n",
    "\n",
    "```shell\n",
    "uv add langchain-ollama\n",
    "```\n",
    "\n",
    "Luego, en el laboratorio, realiza este cambio:\n",
    "\n",
    "```python\n",
    "from langchain_ollama import ChatOllama\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm = ChatOllama(model=\"gemma3:4b\")\n",
    "```\n",
    "\n",
    "Y, por supuesto, ejecuta antes:\n",
    "\n",
    "```shell\n",
    "!ollama pull gemma3:4b\n",
    "```\n",
    "\n",
    "(o el modelo que prefieras).\n",
    "\n",
    "Muchas gracias a **Miroslav P.** por aportar este ejemplo y a **Arvin F.** por la pregunta. üôå\n",
    "\n",
    "---\n",
    "\n",
    "## LangGraph con otros modelos\n",
    "\n",
    "Simplemente sigue la misma receta que arriba, pero usando cualquiera de los modelos disponibles aqu√≠:\n",
    "üëâ [Lista de integraciones de chat de LangChain](https://python.langchain.com/docs/integrations/chat/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGen con otros modelos\n",
    "\n",
    "Aqu√≠ tienes otra contribuci√≥n de **Miroslav P.** (¬°gracias!) para usar **Ollama** junto con modelos locales en **AutoGen**.\n",
    "Miroslav adem√°s comparte un excelente ejemplo mostrando el buen rendimiento del modelo **gemma3**. üöÄ\n",
    "\n",
    "```python\n",
    "# model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    " \n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    " \n",
    "model_client = OllamaChatCompletionClient(\n",
    "    model=\"gemma3:4b\",\n",
    "    model_info={\n",
    "        \"vision\": True,\n",
    "        \"function_calling\": False,\n",
    "        \"json_output\": True,\n",
    "        \"family\": \"unknown\",\n",
    "    },\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vale la pena tener en cuenta üß†\n",
    "\n",
    "1. Si deseas usar **Ollama** para ejecutar modelos localmente, es posible que los modelos m√°s peque√±os tengan dificultades con los proyectos m√°s avanzados. Deber√°s **experimentar con distintos tama√±os y capacidades de modelos**, y tener **mucha paciencia** para encontrar uno que funcione bien.\n",
    "   Es probable que varios de nuestros proyectos sean demasiado exigentes para **llama3.2**.\n",
    "   Como alternativa, considera los **modelos gratuitos en [openrouter.ai](https://openrouter.ai)** o los modelos **muy econ√≥micos**, como **DeepSeek**.\n",
    "\n",
    "2. Los **modelos de chat** suelen rendir mejor que los **modelos de razonamiento**, ya que estos √∫ltimos pueden ‚Äúpensar demasiado‚Äù algunas tareas.\n",
    "   Es importante **experimentar** ‚Äî m√°s grande no siempre significa mejor. üòâ\n",
    "\n",
    "3. Puede resultar confuso, pero existen **dos proveedores con nombres muy parecidos**:\n",
    "\n",
    "   * **Grok** es el LLM de **X (Elon Musk)**\n",
    "   * **Groq** es una **plataforma de inferencia r√°pida** para modelos *open source*\n",
    "\n",
    "Un estudiante me se√±al√≥ que **¬°Groq fue el primero!** üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
